{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading raw Shakespeare texts\n",
      "Tokenizing Shakespeare texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (267688 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollator,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerBase,\n",
    "    Trainer,\n",
    ")\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# detect cuda\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_type = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "max_seq_length = min(tokenizer.model_max_length, 1024)\n",
    "\n",
    "def get_shakespeare_dataset():\n",
    "    char_tknzr = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True).encode\n",
    "    DATA_PATH = os.path.join(os.getcwd(), \"datasets\", \"shakespeare\")\n",
    "    raw_path = os.path.join(DATA_PATH, \"raw.txt\")\n",
    "    train_path = os.path.join(DATA_PATH, f\"train.npy\")\n",
    "    test_path = os.path.join(DATA_PATH, f\"test.npy\")\n",
    "    # if path is not even there, download all data\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        print(\"Downloading raw Shakespeare texts\")\n",
    "        url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "        os.makedirs(DATA_PATH, exist_ok=True)\n",
    "        text = requests.get(url, timeout=60).text\n",
    "        with open(raw_path, \"w+\", encoding=\"utf8\") as f:\n",
    "            f.write(text)\n",
    "    \n",
    "    if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
    "        print(\"Tokenizing Shakespeare texts\")\n",
    "        # load text\n",
    "        with open(raw_path, encoding=\"utf8\") as f:\n",
    "            text = \"\".join(f.readlines())\n",
    "        i = int(0.8*len(text))\n",
    "        # encode text\n",
    "        x = np.array(char_tknzr(text[:i]), dtype=np.uint16)\n",
    "        x_test = np.array(char_tknzr(text[i:]), dtype=np.uint16)\n",
    "        # map memory\n",
    "        mem = np.memmap(train_path, dtype=np.uint16, mode=\"w+\", shape=x.shape)\n",
    "        mem[:] = x\n",
    "        mem = np.memmap(test_path, dtype=np.uint16, mode=\"w+\", shape=x_test.shape)\n",
    "        mem[:] = x_test\n",
    "\n",
    "    # at this point we know that the binfile was properly created so we load it\n",
    "    return {\"train\": np.memmap(train_path, dtype=np.uint16, mode=\"r\"),\n",
    "            \"val\": np.memmap(test_path, dtype=np.uint16, mode=\"r\")}\n",
    "\n",
    "dataset = get_shakespeare_dataset()\n",
    "# sft_config = SFTConfig(\n",
    "#     dataset_text_field=\"text\",\n",
    "#     max_seq_length=512,\n",
    "#     output_dir=\"/tmp\",\n",
    "# )\n",
    "# trainer = SFTTrainer(\n",
    "#     \"gpt2\",\n",
    "#     train_dataset=dataset,\n",
    "#     args=sft_config,\n",
    "# )\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num training tokens: 267688\n",
      "Num validation tokens: 70338\n",
      "[3285  502 2740   13  198  198 3237   25  198 5248]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num training tokens: {len(dataset['train'])}\")\n",
    "print(f\"Num validation tokens: {len(dataset['val'])}\")\n",
    "print(dataset['train'][10:20])\n",
    "\n",
    "dataset_batch_size = 4\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. Ishiguro is back! If you need someone on the ship, please let me know\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello. Ishiguro is\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=20,\n",
    "    pad_token_id=tokenizer.eos_token_id  # EOS Token\n",
    ")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args[\"lr\"] = 1e-3\n",
    "args[\"beta1\"] = 0.9\n",
    "args[\"beta2\"] = 0.999\n",
    "\n",
    "use_fused = (device_type == 'cuda') and ('fused' in inspect.signature(torch.optim.AdamW).parameters)\n",
    "extra_args = dict(fused=True) if use_fused else dict()\n",
    "opt = torch.optim.AdamW(lr=args.lr, betas=(args.beta1, args.beta2),\n",
    "                                weight_decay=args.weight_decay, **extra_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
